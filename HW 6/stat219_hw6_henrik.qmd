---
title: "Homework 6"
format:
  pdf:
    fontsize: 12pt
    geometry:
      - inner=1.5cm
      - outer=1.5cm
      - top=2.5cm
      - bottom=2.5cm
    include-in-header:
      - text: |
          \addtokomafont{disposition}{\rmfamily}
          \usepackage{fancyhdr, lastpage, framed, caption, xcolor}
          \captionsetup[figure]{labelformat=empty}
          \pagestyle{fancyplain}
          \fancyhf{}
          \lhead{\fancyplain{}{STAT 0219: Time Series Analysis}}
          \rhead{\fancyplain{}{Stratton - HW6}}
          \fancyfoot[RO, LE] {page \thepage\ of \pageref{LastPage}}
          \thispagestyle{plain}
editor: source
---
<!-- Note: the YAML header is everything above this line. -->
\vspace{-3in}
\textbf{Name:} Henrik Nelson
\textbf{Due:} 2024/11/04
\vspace{1.5in}

```{r, include = F}
rm(list= ls())
library(tidyverse)
```

Be sure to submit **both** the .pdf and .qmd file to Canvas by Monday, November 4th at 11:59 pm. The purpose of this assignment is to use simulation to better understand why we need to account for serial correlation in a time series. 

For this assignment, you may use the `generate_ts_reg` function contained in the `helpers.R` script to generate time series with trend, seasonality, and autocorrelated errors from an AR(1) process. 

```{r, fig.dim = c(7, 4)}
source("helpers.R")

# example: 50 weeks of data
ex_ts <- generate_ts_reg(
  1027204,
  n = 100*7,
  freq = 7,
  betas = c(-1, .05, rnorm(6, sd = .5)) # beta0, beta1, and 6 harmonic cycles
)
str(ex_ts)

ex_ts$df %>%
  ggplot() + 
  geom_line(aes(x = t, y = y)) +
  theme_bw()

lm(y ~ ., ex_ts$df)
ex_ts$params$betas
```

\newpage

**Question 1 [11 pt]** The goal of this assignment is to conduct a *simulation study* that demonstrates why we need to account for serial autocorrelation when fitting regression models. The purpose of this first question is to get our feet wet with the idea of a simulation study. 

1. [2 pt] Use the `generate_ts_reg` function to generate a time series that represents 20 years of monthly data. Set the `beta` vector equal to `c(-1, 0.05, 1, -1, rnorm(10, sd = .5))` and the autocorrelation parameter to `.8`. Plot the resulting time series. 

```{r, indent = "   ", fig.dim = c(7, 4)}
ts_data <- generate_ts_reg(seed =123, n= 240, freq = 12, betas = c(-1, 0.05, 1, -1, rnorm(10, sd = .5)), alpha = 0.8)
ts_data$df |>
  ggplot() + 
  geom_line(aes(x = t, y = y)) +
  theme_bw() +
  labs (title = "Simulated 20 year time series", x="Time value (Months)", y="Value")

```

2. [2 pt] Fit a linear regression model that includes the time index and all 12 harmonic seasonal cycles. Create a PACF plot of the residuals and comment on how the ACF plot relates to the way you generated the series. 

```{r, indent = "   ", fig.dim = c(7, 4)}
lin_reg_model <-lm(y ~ t + sin1t + cos1t + sin2t + cos2t + sin3t + cos3t + 
             sin4t + cos4t + sin5t + cos5t + sin6t + cos6t, data = ts_data$df)
summary(lin_reg_model)
acf(resid(lin_reg_model))
pacf(resid(lin_reg_model))
```

   :::{.callout-warning icon=false appearance="simple"}
   The PACF plot suggests that the residuals are an AR(1) series with $\alpha = .8$, which makes sense because that is exactly what we simulated. 
   :::

3. [2 pt] Create confidence intervals for the regression coefficients using `confint` and determine which intervals captured the generating parameters (which you can find contained within the output of the `generate_ts_reg` function). 

```{r, indent = "   "}
confint(lin_reg_model)
ts_data$params$betas
```

   :::{.callout-warning icon=false appearance="simple"}
    The conf intervals captured the generating parameters for the intercept, t, sin2t, and cos4t
   :::
   
4. [4 pt] Fit a GLS model with an AR(1) correlation structure, create confidence intervals for the regression coefficients, and again determine which intervals captured the generating values. 

```{r, warning = F, message = F, indent = "   "}
library(nlme)
gls_fit <- gls(y ~ t + sin1t + cos1t + sin2t + cos2t + sin3t + cos3t + 
       sin4t + cos4t + sin5t + cos5t + sin6t + cos6t, data = ts_data$df,
  correlation = corARMA(p = 1, q = 0))
summary(gls_fit)
print(intervals(gls_fit)$coef)
```

   :::{.callout-warning icon=false appearance="simple"}
It seems that the GLS model may have done a bit better capturing all the generating values except sin1t, sin5t, and sin6t.
   :::
   
5. [1 pt] If we were to repeatedly simulate time series like in question 1-1 through 1-4 and calculate 95% confidence intervals for the regression coefficients in the model, approximately what percent of the constructed intervals should capture the generating values, if we are appropriately modeling the uncertainty?

   :::{.callout-warning icon=false appearance="simple"}
If we were to repeatedly simulate time series data and calculate 95% confidence intervals for the regression coefficients, we would expect about 95% of these intervals to capture the true generating values. This is because a 95% confidence interval is designed to contain the true parameter in 95% of samples, assuming the model accurately reflects the data-generating process. With enough simulations, the capture rate should approach 95%.
   :::
   
\newpage

**Question 2 [10 pt]** We are now ready to conduct our simulation study. To do so, repeat the following process 100 times:

- simulate new data with a distinct seed (using the same `betas` and `alpha` as before)
- fit an OLS model and determine for which parameters the confidence interval captures the generating values, store these values
- fit a GLS model and determine for which parameters the confidence interval captures the generating values, store these values

Then, calculate the proportion of times (out of 100 simulations) that each model captured the generating values for each of the regression coefficients. Create a visual that displays the resulting proportions and comment on what the results suggest about the importance of accounting for serial autocorrelation when estimating regression coefficients. 

```{r, eval = F}

## after writing in the models, had some trouble storing values into dataframe so used ChatGPT to help 
set.seed(1104)
all_results <- vector("list", 100)

for (i in 1:100) {
  ts_100sim_data <- generate_ts_reg(seed = i + 1104, n = 240, freq = 12,
    betas = c(-1, 0.05, 1, -1, rnorm(10, sd = 0.5)), alpha = 0.8)
  
  true_betas <- ts_100sim_data$params$betas
  
  names(true_betas) <- c("(Intercept)", "t", "sin1t", "cos1t", "sin2t", "cos2t",
                         "sin3t", "cos3t", "sin4t", "cos4t", "sin5t", "cos5t", 
                         "sin6t", "cos6t")
  
  ols_100sim_model <- lm( y ~ t + sin1t + cos1t + sin2t + cos2t + sin3t + cos3t + 
          sin4t + cos4t + sin5t + cos5t + sin6t + cos6t, data = ts_100sim_data$df)
  ols_confint <- confint(ols_100sim_model)
  ols_coefs <- coef(ols_100sim_model)
  
  gls_100sim_fit <- gls(y ~ t + sin1t + cos1t + sin2t + cos2t + sin3t + cos3t + 
         sin4t + cos4t + sin5t + cos5t + sin6t + cos6t,data = ts_100sim_data$df,
    correlation = corARMA(p = 1, q = 0))
  gls_confint <- intervals(gls_100sim_fit)$coef
  gls_coefs <- coef(gls_100sim_fit)
  
    iteration_results <- data.frame(
      Simulation = i,
      Model = rep(c("OLS", "GLS"), each = length(common_coefs)),
      Coefficient = rep(common_coefs, 2),
      Lower = c(ols_confint[common_coefs, 1], gls_confint[common_coefs, "lower"]),
      Estimate = c(ols_coefs[common_coefs], gls_coefs[common_coefs]),
      Upper = c(ols_confint[common_coefs, 2], gls_confint[common_coefs, "upper"]),
      TrueValue = rep(true_betas[common_coefs], 2),
      Captured = c(
        (true_betas[common_coefs] >= ols_confint[common_coefs, 1]) & 
        (true_betas[common_coefs] <= ols_confint[common_coefs, 2]),
        (true_betas[common_coefs] >= gls_confint[common_coefs, "lower"]) & 
        (true_betas[common_coefs] <= gls_confint[common_coefs, "upper"])
      )
    )
    
    # Append iteration results to list
    all_results[[i]] <- iteration_results
}
results <- do.call(rbind, all_results)
save(results, file = "sim.rdata")
```

```{r, fig.dim = c(7, 6)}
# then load it here: load("sim.rdata")
load("sim.rdata")
head(results)
summary(results)

capture_proportions <- results |>
  group_by(Model, Coefficient) |>
  summarize(Capture_Proportion = mean(Captured), .groups = "drop")

print(capture_proportions)

ggplot(capture_proportions, aes(x = Coefficient, y = Capture_Proportion, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Capture Proportions of True Parameter Values by Model",
       x = "Coefficient",
       y = "Proportion of Captured Intervals") +
  theme_minimal()
```

:::{.callout-warning icon=false appearance="simple"}
The results highlight the importance of accounting for serial autocorrelation when estimating regression coefficients in time series data. The GLS model shows much higher capture rates for key coefficients, like the intercept, t, and periodic terms (sin1t, cos6t, sin6t), compared to OLS. This indicates that GLS provides more accurate confidence intervals that better capture the true parameter values in autocorrelated data. In contrast, the OLS model, which ignores autocorrelation, has lower capture rates and narrower confidence intervals, making it less reliable for inference. Overall, these findings underscore that incorporating autocorrelation through GLS results in more accurate and trustworthy estimates.
:::
   
   