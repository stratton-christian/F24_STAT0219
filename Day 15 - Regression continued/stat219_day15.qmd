---
title: "Day 15 - Time Series Regression "
format:
  pdf:
    fontsize: 12pt
    geometry:
      - inner=1.5cm
      - outer=1.5cm
      - top=2.5cm
      - bottom=2.5cm
    include-in-header:
      - text: |
          \addtokomafont{disposition}{\rmfamily}
          \usepackage{fancyhdr, lastpage, framed, caption, xcolor, setspace}
          \captionsetup[figure]{labelformat=empty}
          \pagestyle{fancyplain}
          \fancyhf{}
          \lhead{\fancyplain{}{STAT 0219: Time Series Analysis}}
          \rhead{\fancyplain{}{Stratton - Day 15}}
          \fancyfoot[RO, LE] {page \thepage\ of \pageref{LastPage}}
          \thispagestyle{plain}
editor: source
---
\vspace{-1in}

```{r, include = F}
library(tidyverse)
```

## Introduction

Today we learn how to formally fit a time series regression model by combining a regression model with a serially correlated error process. 

***

\newpage

## Review

The code below creates a time series regression model

```{r}
# data
data("AirPassengers")
ap <- AirPassengers
ap_tbl <- tibble(
  ap = c(ap), year = rep(1949:1960, each = 12),
  month = rep(1:12, 12) %>% factor()
) %>% mutate(t = 1:n(), t2 = t^2)  %>%
  mutate(t_scaled = c(scale(t)), t2_scaled = c(scale(t2))) %>%
  mutate(log_ap = log(ap))
ap_sub_tbl <- ap_tbl %>% filter(year < 1960)

# fit model
ols_fit <- lm(log_ap ~ t + t2 + month, ap_sub_tbl)
```

\newpage

## Generalized least squares overview

:::{.callout-note title="GLS theory"}
A ______________________ is a model of the form
$$
y_i = \beta_0 + \beta_1x_{i,1} + \beta_2x_{i,2} + \dots + \beta_px_{i,p} + \epsilon_i
$$
where $\epsilon_i$ is ________________________________ distributed $N(0, \sigma^2)$. In matrix notation, the above model is equivalent to 
$$
\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
$$
where
$$
\boldsymbol{y} = \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}, \hspace{5mm}
\boldsymbol{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \dots & x_{1p} \\
1 & x_{21} & x_{22} & \dots & x_{2p}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
1 & x_{n1}& x_{n2} & \dots & x_{np}
\end{bmatrix}, \hspace{5mm}
\boldsymbol{\beta} = \begin{bmatrix}
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_p
\end{bmatrix}, \hspace{5mm}
\boldsymbol{\epsilon} = \begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_n
\end{bmatrix}
$$
By properties of normal distributions, the above model is equivalent to
$$
\boldsymbol{y} \sim \mathcal{N} (\boldsymbol{X}\boldsymbol{\beta}, \boldsymbol{\Sigma})
$$
where $\boldsymbol{\Sigma} = \sigma^2 \boldsymbol{\mathcal{I}}_n$ and $\boldsymbol{\mathcal{I}}_n$ is an $n \times n$ identity matrix. To fit the model, we must estimate ___________ and _____________. The estimated regression equation is written as:
$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_{i,1} + \hat{\beta}_2x_{i,2} + \dots + \hat{\beta}_px_{i,p}
$$
Parameter estimates are obtained by minimizing the ________________________. The error in regression is called a _______________, 
$$
e_i = y_i - \hat{y}_i
$$
The _____________________________, which minimizes the __________, is 
$$
\hat{\boldsymbol{\beta}} = (\boldsymbol{X}^\top\boldsymbol{X})^{-1}\boldsymbol{X}^\top\boldsymbol{y}
$$
:::

## Problems

- Estimating GLS models is *very* hard
- No closed for expression for the standard error of the prediction
- Solutions:
   - Delta method
   - Bootstrapping
   - Use Bayes


\newpage

## State-space model using `arima`