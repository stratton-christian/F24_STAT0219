---
title: "Day 15 - Time Series Regression "
format:
  pdf:
    fontsize: 12pt
    geometry:
      - inner=1.5cm
      - outer=1.5cm
      - top=2.5cm
      - bottom=2.5cm
    include-in-header:
      - text: |
          \addtokomafont{disposition}{\rmfamily}
          \usepackage{fancyhdr, lastpage, framed, caption, xcolor, setspace}
          \captionsetup[figure]{labelformat=empty}
          \pagestyle{fancyplain}
          \fancyhf{}
          \lhead{\fancyplain{}{STAT 0219: Time Series Analysis}}
          \rhead{\fancyplain{}{Stratton - Day 15}}
          \fancyfoot[RO, LE] {page \thepage\ of \pageref{LastPage}}
          \thispagestyle{plain}
editor: source
---
\vspace{-1in}

```{r, include = F}
library(tidyverse)
```

## Introduction

Today we learn how to formally fit a time series regression model by combining a regression model with a serially correlated error process. 

***

\newpage

## Review

The code below fits a time series regression model to the `AirPassengers` series before the year 1960 that models the log count of air passengers by the interaction of month and time index and time index squared.

```{r, fig.dim = c(7,3.5)}
# data
data("AirPassengers")
ap <- AirPassengers
ap_tbl <- tibble(
  ap = c(ap), year = rep(1949:1960, each = 12),
  month = rep(1:12, 12) %>% factor()
) %>% mutate(t = 1:n(), t2 = t^2)  %>%
  mutate(t_scaled = c(scale(t)), t2_scaled = c(scale(t2))) %>%
  mutate(log_ap = log(ap))
ap_sub_tbl <- ap_tbl %>% filter(year < 1960)

# fit model
ols_fit <- lm(log_ap ~ t_scaled*month + t2_scaled , ap_sub_tbl)
confint(ols_fit)
summary(ols_fit)
acf(resid(ols_fit))
pacf(resid(ols_fit))
```

\newpage

## Generalized least squares overview

:::{.callout-note title="GLS theory"}
Suppose we extend the typical _________________ model to accomodate violations of independence and constant variance. That is, let
$$
y_i = \beta_0 + \beta_1x_{i,1} + \beta_2x_{i,2} + \dots + \beta_px_{i,p} + \epsilon_i
$$
where $\epsilon_i$ is distributed $N(0, \sigma_i^2)$ and may not be indendent. In matrix notation, the above model is equivalent to 
$$
\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
$$
where
$$
\boldsymbol{y} = \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}, \hspace{5mm}
\boldsymbol{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \dots & x_{1p} \\
1 & x_{21} & x_{22} & \dots & x_{2p}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
1 & x_{n1}& x_{n2} & \dots & x_{np}
\end{bmatrix}, \hspace{5mm}
\boldsymbol{\beta} = \begin{bmatrix}
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_p
\end{bmatrix}, \hspace{5mm}
\boldsymbol{\epsilon} = \begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_n
\end{bmatrix}
$$
By properties of normal distributions, the above model is equivalent to
$$
\boldsymbol{y} \sim \mathcal{N} (\boldsymbol{X}\boldsymbol{\beta},\boldsymbol{\Sigma})
$$
where 

$$
\boldsymbol{\Sigma} = \begin{bmatrix}
\sigma_{11} & \sigma_{12} & \dots & \sigma_{1n} \\
\sigma_{21} & \sigma_{22} & \dots & \sigma_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{n1} & \sigma_{n2} & \dots & \sigma_{nn}
\end{bmatrix}
$$
and $\boldsymbol{\Sigma} = \boldsymbol{\Sigma}^{\top}$. In its general form, we cannot _________________ because there are more parameters than observations (estimating $\boldsymbol{\Sigma}$ requires estimating $n + \frac{n(n-1)}{2}$ parameters). Therefore, to estimate $\boldsymbol{\Sigma}$, we must assume that some structure exists. An example of such structure is an ___________ process, where:
$$
\boldsymbol{\Sigma} = \sigma^2 \begin{bmatrix}
1 & \rho & \rho^2 & \rho^3 & \dots & \rho^{n-1} \\
\rho & 1 & \rho & \rho^2 & \dots & \rho^{n-2} \\
\rho & \rho^2 & 1 & \rho & \dots & \rho^{n-3} \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
\rho & \rho^2 & \rho^3 & \rho^4 & \dots & 1 
\end{bmatrix}
$$

The ____________________________ estimate of $\boldsymbol{\beta}$ is
$$
\hat{\beta}_{gls} = \left(\boldsymbol{X}^\top \boldsymbol{\Omega}^{-1} \boldsymbol{X} \right)^{-1} \boldsymbol{X}^\top \boldsymbol{\Omega}^{-1} \boldsymbol{y}
$$

Estimating the other model parameters (ex. $\sigma^2$ and $\rho$) can be quite difficult and typically uses a process called restricted maximum likelihood.
:::

\newpage

## GLS in R

:::{.callout-note}
To implement GLS in R, we use the `gls` function in the `nlme` package. 
:::

```{r, echo = T, message = F, warning = F, fig.dim = c(7, 4)}
library(nlme)
gls_fit <- gls(
  log_ap ~ t_scaled*month + t2_scaled, correlation = corARMA(p = 1, q = 0), ap_sub_tbl
)
summary(gls_fit)
confint(gls_fit)

# normalized residuals account for the estimate serial correlation
acf(resid(gls_fit, type = "normalized"))
pacf(resid(gls_fit, type = "normalized"))
```

\newpage

## Forecasts with GLS

:::{.callout-note}
To obtain forecasts, we use the `predict` function. 
:::

```{r, echo = T, message = F, warning = F, fig.dim = c(7, 4)}
# create table for prediction
pred_tbl <- ap_tbl %>%
  filter(year >= 1960) 

# obtain prediction
## note: you cannot obtain SEs!!!
(pred <- predict(gls_fit, pred_tbl))

# plot them
ap_tbl %>%
  mutate(
    fitted = c(fitted(gls_fit), rep(NA, 12)),
    forecast = c(rep(NA, 132), pred)
  ) %>%
  pivot_longer(log_ap:forecast, names_to = "type", values_to = "val") %>%
  mutate(dt = ymd(paste0(year, "-", month, "-1"))) %>%
  ggplot() + 
  geom_line(aes(x = dt, y = val, col = type)) +
  theme_bw()
```


:::{.callout-important}
Unfortunately, there is no closed form solution for the _______________________ in a GLS model. Therefore, to obtain estimates of uncertainty in our forecasts from a GLS model, you must:

- Figure out how to bootstrap the SE of the prediction (disgusting)
- Use the Delta method (somehow more disgusting)
- Fit a Bayesian model 
:::

:::{.callout-note title="Why use GLS?"}
GLS is most useful for making inference about regression coefficients with complicated correlation structures (GLS can accommodate hierarchical models, time series models, longitudinal models, and any combination of the three). GLS allows us to adjust the SEs of the coefficient estimates and obtain confidence intervals and p-values that account for the serial correlation that is present.  
:::


\newpage

## State-space model using `arima`

:::{.callout-note title="State-space models"}
The `arima` function offers a way to fit time series regression models and obtain estimates of the _________________________ using a ____________ representation of the model. The details are beyond the scope of this class, but representing the model in this way allows the model to be estimated using a Kalman-filter, which enables estimates of the ___________________. 
:::

```{r, message = F, warning = F, fig.dim = c(7, 3)}
# prepare a few things
ts <- ts(
  ap_sub_tbl$log_ap,
  start = c(1949, 1),
  freq = 12
)

ss_fit <- arima(
  x = ts,
  order = c(1, 0, 0),
  xreg = model.matrix(
    ~ t_scaled*month + t2_scaled,
    ap_sub_tbl
  ),
  include.mean = F
)
ss_fit

acf(ss_fit$residuals)
pacf(ss_fit$residuals)
```

\newpage

## Forecasts with state-space models

:::{.callout-note title="Forecasts with state-space models"}
To obtain forecasts, we again use the `predict` function, specifying the number of time points ahead (`n.ahead`) and the matrix of new regression coefficients (`newxreg`).
:::

```{r, message = F, warning = F, fig.dim = c(7, 5)}
# forecasts
ss_fitted <- predict(
  ss_fit, 
  newxreg = model.matrix(
    ~ t_scaled*month + t2_scaled,
    ap_sub_tbl
  )
)

(ss_pred <- predict(
  ss_fit, n.ahead = 12, 
  newxreg = model.matrix(
    ~ t_scaled*month + t2_scaled,
    ap_tbl %>% filter(year >= 1960)
  )
))

# plot
ap_tbl %>%
  mutate(
    fitted = c(ss_fitted$pred, rep(NA, 12)),
    forecast = c(rep(NA, 132), ss_pred$pred)
  ) %>%
  pivot_longer(log_ap:forecast, names_to = "type", values_to = "val") %>%
  mutate(dt = ymd(paste0(year, "-", month, "-1"))) %>%
  ggplot() + 
  geom_line(aes(x = dt, y = val, col = type)) +
  geom_ribbon(
    data = tibble(
      dt = ymd(paste0(rep(1960, 12), "-", 1:12, "-1")),
      lwr = c(ss_pred$pred - 2*ss_pred$se),
      upr = c(ss_pred$pred + 2*ss_pred$se),
      type = "forecast"
    ),
    aes(x = dt, ymin = lwr, ymax = upr),
    alpha = .30
  ) +
  theme_bw()
```

\newpage

## Some comparisons

```{r}
tibble(
  coef = names(coef(ols_fit)),
  ols = coef(ols_fit),
  ols_se = summary(ols_fit)$coefficients[,2],
  gls = coef(gls_fit),
  gls_se = sqrt(diag(summary(gls_fit)$varBeta)),
  ss = ss_fit$coef[-1],
  ss_se = sqrt(diag(ss_fit$var.coef))[-1]
) %>%
  print(n = "all")
```
